{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "T8XzkBT9TvML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements DDPM from the paper https://arxiv.org/abs/2006.11239.\n",
        "\n",
        "It is based on the original implementation by the author: https://github.com/hojonathanho/diffusion and on the pytorch implementation by Phil Wang: https://github.com/lucidrains/denoising-diffusion-pytorch.\n",
        "\n",
        "It uses the Flowers102 dataset: https://www.robots.ox.ac.uk/~vgg/data/flowers/102/."
      ],
      "metadata": {
        "id": "0w4W-MLSTw0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpGluX9fjWHL",
        "outputId": "042c20cf-3e84-4f23-bcf8-320e98dd54ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m662.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsBIpY1PQm18"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from einops import rearrange\n",
        "from torch import nn, einsum\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "7j1XWdubURqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sinusoidal Position Embeddings"
      ],
      "metadata": {
        "id": "xlaaU0PlUVpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddings(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "\n",
        "  def forward(self, timestamps):\n",
        "    # Based on fairseq implementation:\n",
        "    # https://github.com/facebookresearch/fairseq/blob/main/fairseq/modules/sinusoidal_positional_embedding.py\n",
        "    device = timestamps.device\n",
        "    half_dim = self.dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "    emb = timestamps[:, None] * emb[None, :]\n",
        "    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "_zJoPsCoTgFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group Normalization"
      ],
      "metadata": {
        "id": "WiEtbZmfUa2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1, 8\n",
        "groups = 8"
      ],
      "metadata": {
        "id": "yDfYtMfQg1qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(num_channels, num_groups=groups):\n",
        "  return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)"
      ],
      "metadata": {
        "id": "Cyh390XyacBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Swish (aka SiLU) nonlinearity activation function"
      ],
      "metadata": {
        "id": "uNPl3W9uUhV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(inplace=False):\n",
        "  return torch.nn.SiLU(inplace=inplace)"
      ],
      "metadata": {
        "id": "8YTBb7w1UkSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time Embedding layer"
      ],
      "metadata": {
        "id": "k-SiMxiIUX5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeMLP(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "    self.pos_emb = PositionEmbeddings(in_dim)\n",
        "    self.linear1 = nn.Linear(in_dim, out_dim)\n",
        "    self.linear2 = nn.Linear(out_dim, out_dim)\n",
        "    self.act = swish()\n",
        "\n",
        "  def forward(self, x):\n",
        "    temb = self.pos_emb(x)\n",
        "    temb = self.linear1(temb)\n",
        "    temb = self.act(temb)\n",
        "    temb = self.linear2(temb)\n",
        "    return temb"
      ],
      "metadata": {
        "id": "FB9W791fUaX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upsample block"
      ],
      "metadata": {
        "id": "2FZN79JYUtWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.upsample = nn.Upsample(scale_factor=2.0, mode='nearest')\n",
        "    self.conv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    x = self.conv(x)\n",
        "    assert x.shape == (B, C, H*2, W*2)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "SVTPAdgzUuuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downsample block"
      ],
      "metadata": {
        "id": "oG3Sd5SWU-Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownsampleBlock(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(dim, dim, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    x = self.conv(x)\n",
        "    assert x.shape == (B, C, H // 2, W // 2)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "dm_ptvmvU_Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ResNet block"
      ],
      "metadata": {
        "id": "xkTvB6ySU_jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, time_emb_dim, num_groups=groups):\n",
        "    super().__init__()\n",
        "    self.act = swish()\n",
        "\n",
        "    self.norm1 = normalize(num_groups=num_groups, num_channels=dim_in)\n",
        "    self.conv1 = nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "    self.norm2 = normalize(num_groups=num_groups, num_channels=dim_out)\n",
        "    self.conv2 = nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "    self.linear = nn.Linear(time_emb_dim, dim_out)\n",
        "    self.dim_out = dim_out\n",
        "    self.conv_match = nn.Conv2d(dim_in, dim_out, kernel_size=1)\n",
        "\n",
        "  def forward(self, x, temb):\n",
        "    # normalize -> nonlinearity -> conv\n",
        "    h = x\n",
        "    h = self.norm1(h)\n",
        "    h = self.act(h)\n",
        "    h = self.conv1(h)\n",
        "\n",
        "    # makes sure x shape will match with h shape\n",
        "    if x.shape[1] != self.dim_out:\n",
        "      x = self.conv_match(x)\n",
        "\n",
        "    # add timestep embedding\n",
        "    timestep_emb = self.act(temb)\n",
        "    timestep_emb = self.linear(timestep_emb)\n",
        "    timestep_emb = timestep_emb[:, :, None, None] # B, C, 1, 1\n",
        "\n",
        "    h = h + timestep_emb\n",
        "\n",
        "    # normalize -> nonlinearity -> conv\n",
        "    h = self.norm2(h)\n",
        "    h = self.act(h)\n",
        "    h = self.conv2(h)\n",
        "    assert h.shape == x.shape\n",
        "\n",
        "    # add residual\n",
        "    h = h + x\n",
        "    return h"
      ],
      "metadata": {
        "id": "aZvnhuD2VCoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention blocks"
      ],
      "metadata": {
        "id": "aLFAEij_VD1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from Phil Wang's implementation\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, heads=4, dim_head=32):\n",
        "    super().__init__()\n",
        "    self.scale = dim_head**-0.5\n",
        "    self.heads = heads\n",
        "\n",
        "    hidden_dim = dim_head * heads\n",
        "    self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "    self.conv = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv)\n",
        "\n",
        "    # Attention = softmax(Q*K_T / sqrt(d_k))*V\n",
        "    # qxk = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "    qxk = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "    qxk = qxk * self.scale\n",
        "    qxk = qxk - qxk.amax(dim=-1, keepdim=True).detach()\n",
        "    att = qxk.softmax(dim=-1)\n",
        "\n",
        "    att = einsum(\"b h i j, b h d j -> b h i d\", att, v)\n",
        "    att = rearrange(att, \"b h (x y) d -> b (h d) x y\", x=H, y=W)\n",
        "    return self.conv(att)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "  def __init__(self, dim, heads=4, dim_head=32):\n",
        "    super().__init__()\n",
        "    self.scale = dim_head**-0.5\n",
        "    self.heads = heads\n",
        "\n",
        "    hidden_dim = dim_head * heads\n",
        "    self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "    self.conv = nn.Conv2d(hidden_dim, dim, 1)\n",
        "    self.norm = normalize(num_channels=dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv)\n",
        "\n",
        "    q = q.softmax(dim=-2)\n",
        "    k = k.softmax(dim=-1)\n",
        "\n",
        "    q = q * self.scale\n",
        "    context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "    att = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "    att = rearrange(att, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=H, y=W)\n",
        "    att = self.conv(att)\n",
        "    att = self.norm(att)\n",
        "    return att"
      ],
      "metadata": {
        "id": "0_N06IcYVElp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### U-Net model"
      ],
      "metadata": {
        "id": "DPWr0iAQVGVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_dim,\n",
        "               widths,\n",
        "               channels=3,\n",
        "               channel_mult=(1, 2, 4, 8),\n",
        "               add_attention=[False, False, True, True]):\n",
        "    super().__init__()\n",
        "\n",
        "    in_dims = [in_dim] + widths[:-1]\n",
        "    out_dims = widths\n",
        "\n",
        "    # Initial convolution\n",
        "    self.first_conv = nn.Conv2d(channels, in_dim, kernel_size=3, padding='same')\n",
        "\n",
        "    # Timestep embedding\n",
        "    self.time_mlp = TimeMLP(in_dim, in_dim * 4)\n",
        "    time_dim = in_dim * 4\n",
        "\n",
        "    # Downsampling\n",
        "    self.downs = nn.ModuleList([])\n",
        "    for i in range(len(widths)):\n",
        "      self.downs.append(\n",
        "          nn.ModuleList([\n",
        "              ResNetBlock(in_dims[i], out_dims[i], time_emb_dim=time_dim),\n",
        "              LinearAttention(out_dims[i]) if add_attention[i] else nn.Identity(),\n",
        "              DownsampleBlock(out_dims[i]) if out_dims[i] != out_dims[-1] else nn.Identity()\n",
        "          ])\n",
        "      )\n",
        "\n",
        "    # Middle\n",
        "    self.middle_block1 = ResNetBlock(out_dims[-1], out_dims[-1], time_emb_dim=time_dim)\n",
        "    self.middle_attn = Attention(out_dims[-1])\n",
        "    self.middle_block2 = ResNetBlock(out_dims[-1], out_dims[-1], time_emb_dim=time_dim)\n",
        "\n",
        "    # Upsampling\n",
        "    self.ups = nn.ModuleList([])\n",
        "    for i in reversed(range(len(widths))):\n",
        "      self.ups.append(\n",
        "          nn.ModuleList([\n",
        "              ResNetBlock(out_dims[i] * 2, in_dims[i], time_emb_dim=time_dim),\n",
        "              LinearAttention(in_dims[i]) if add_attention[i] else nn.Identity(),\n",
        "              UpsampleBlock(in_dims[i]) if i != 0 else nn.Identity()\n",
        "          ])\n",
        "      )\n",
        "\n",
        "    # Final layer\n",
        "    self.final_norm = normalize(num_channels=in_dims[0])\n",
        "    self.final_act = swish()\n",
        "    self.final_conv = nn.Conv2d(in_dim, channels, 1)\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    x = self.first_conv(x)\n",
        "\n",
        "    # Timestep embedding\n",
        "    t = self.time_mlp(t)\n",
        "    hs = []\n",
        "\n",
        "    # Downsampling\n",
        "    for block, attn, down_sample in self.downs:\n",
        "      x = block(x, t)\n",
        "      x = attn(x)\n",
        "      hs.append(x)\n",
        "      x = down_sample(x)\n",
        "\n",
        "    # Middle\n",
        "    x = self.middle_block1(x, t)\n",
        "    x = self.middle_attn(x)\n",
        "    x = self.middle_block2(x, t)\n",
        "\n",
        "    # Upsampling\n",
        "    for block, attn, up_sample in self.ups:\n",
        "      x = torch.cat((x, hs.pop()), dim=1)\n",
        "      x = block(x, t)\n",
        "      x = attn(x)\n",
        "      x = up_sample(x)\n",
        "\n",
        "    # Last layer\n",
        "    x = self.final_norm(x)\n",
        "    x = self.final_act(x)\n",
        "    x = self.final_conv(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "GLNfLuUoVHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion utilities"
      ],
      "metadata": {
        "id": "HaA4WTlOVoYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_1 = 1e-4\n",
        "beta_T = 0.02\n",
        "timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_1, beta_T, timesteps)"
      ],
      "metadata": {
        "id": "97HSgxbcVRnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)"
      ],
      "metadata": {
        "id": "_85_obgvVWc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diffusion: q(x_t | x_{t-1})\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)\n",
        "sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod - 1)\n",
        "\n",
        "# posterior: q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
      ],
      "metadata": {
        "id": "6mGrYRnPVX6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extract coefficients at specified timesteps,\n",
        "then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "\"\"\"\n",
        "def extract(a, t, x_shape):\n",
        "  batch_size = t.shape[0]\n",
        "  out = a.gather(-1, t.cpu())\n",
        "  return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ],
      "metadata": {
        "id": "Z4xpim4osYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward process, add noise\n",
        "def q_sample(x_start, t, noise=None):\n",
        "  if noise is None:\n",
        "    noise = torch.randn_like(x_start)\n",
        "\n",
        "  return extract(sqrt_alphas_cumprod, t, x_start.shape) * x_start + \\\n",
        "         extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise"
      ],
      "metadata": {
        "id": "paPhlJdys0xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(model, x_start, t, noise=None):\n",
        "  if noise is None:\n",
        "    noise = torch.randn_like(x_start)\n",
        "\n",
        "  x_noisy = q_sample(x_start, t, noise)\n",
        "  pred_noise = model(x_noisy, t)\n",
        "\n",
        "  # MSE loss\n",
        "  # loss = F.mse_loss(noise, pred_noise)\n",
        "\n",
        "  # huber loss\n",
        "  loss = F.smooth_l1_loss(noise, pred_noise)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "PC6vQdsgtPjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse process\n",
        "def p_sample(model, x, t, t_index):\n",
        "  betas_t = extract(betas, t, x.shape)\n",
        "  sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "  sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "      sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "  )\n",
        "\n",
        "  pred_noise = model(x, t)\n",
        "  # Equation 11 from paper\n",
        "  model_mean = sqrt_recip_alphas_t * (\n",
        "      x - betas_t * pred_noise / sqrt_one_minus_alphas_cumprod_t\n",
        "  )\n",
        "\n",
        "  if t_index == 0:\n",
        "    return model_mean\n",
        "\n",
        "  posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "  noise = torch.randn_like(x)\n",
        "  return model_mean + torch.sqrt(posterior_variance_t) * noise"
      ],
      "metadata": {
        "id": "I-3hiSr_tzjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "zprN3A08VcMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "image_size = 64"
      ],
      "metadata": {
        "id": "X5ioqeqPu5C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize([image_size, image_size]),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "data = torchvision.datasets.Flowers102(root='./data',\n",
        "                                             #split='train',\n",
        "                                             download=True,\n",
        "                                             transform=transform)\n",
        "data_loader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "p8wOdW6dqc7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "id": "nY4Fnen0wInd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_image(img):\n",
        "  min = img.min()\n",
        "  max = img.max()\n",
        "\n",
        "  img = (img - min) / (max - min)\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "ZrvHqk67wVVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_samples = []\n",
        "N = 25\n",
        "\n",
        "for i in range(N):\n",
        "  data_samples.append(data.__getitem__(i))"
      ],
      "metadata": {
        "id": "zmYKqpChvMtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(figsize=(5, 5), layout='constrained')\n",
        "axs.set_axis_off()\n",
        "\n",
        "for i in range(N):\n",
        "  fig.add_subplot(5, 5, i+1)\n",
        "\n",
        "  temp = data_samples[i]\n",
        "  image = convert_image(temp[0])\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G8idHQckwVHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "dXvyBYLlYpNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "Qi1f8RDtvZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 300\n",
        "lr = 2e-4\n",
        "first_channels = 64\n",
        "channel_mult = (1, 2, 4, 8)\n",
        "input_channels = 3\n",
        "\n",
        "widths = [first_channels * mult for mult in channel_mult]\n",
        "add_attention=[False, False, True, True]"
      ],
      "metadata": {
        "id": "0_b7MOlXvLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "y-zaZVC0Vch6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(\n",
        "    in_dim=image_size,\n",
        "    widths=widths,\n",
        "    channels=input_channels,\n",
        "    channel_mult=channel_mult,\n",
        "    add_attention=add_attention\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "qReQed4cqxhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "N3fkEp0AxW07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, epochs, data_loader):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    curr_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(data_loader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = len(batch[0])\n",
        "      images = batch[0].to(device)\n",
        "      # labels = batch[1].to(device)\n",
        "\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, images, t)\n",
        "      curr_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"Epoch: {epoch+1} | loss: {curr_loss}\")"
      ],
      "metadata": {
        "id": "K14OPL21qny1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, EPOCHS, data_loader)"
      ],
      "metadata": {
        "id": "1yucaOJlx6y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "y4jVPcoSVuIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"\""
      ],
      "metadata": {
        "id": "G6WGNR8tVvqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape, timesteps):\n",
        "  device = next(model.parameters()).device\n",
        "\n",
        "  B = shape[0]\n",
        "\n",
        "  img = torch.randn(shape, device=device)\n",
        "  imgs = []\n",
        "\n",
        "  for i in reversed(range(0, timesteps)):\n",
        "    img = p_sample(model, img, torch.full((B,), i, device=device, dtype=torch.long), i)\n",
        "    imgs.append(img.cpu().numpy())\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "PwOc1v-h3vmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size, channels, timesteps):\n",
        "  return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size), timesteps=timesteps)"
      ],
      "metadata": {
        "id": "3DXPg_ar3oub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch_size = 64"
      ],
      "metadata": {
        "id": "KZyyh8ty4rns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sample(model, image_size=image_size, batch_size=sample_batch_size, channels=input_channels, timesteps=timesteps)"
      ],
      "metadata": {
        "id": "28N2yM_qzLF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "cols = 5\n",
        "rows = 3\n",
        "\n",
        "indices = random.sample(list(range(sample_batch_size)), cols*rows)\n",
        "\n",
        "for i in range(0, timesteps):\n",
        "  if (i + 1) % 20 == 0:\n",
        "    fig = plt.figure(figsize=(cols, rows), layout='constrained')\n",
        "\n",
        "    # Create grid of images\n",
        "    for j in range(cols*rows):\n",
        "      plt.subplot(rows, cols, j+1)\n",
        "\n",
        "      index = indices[j]\n",
        "      img = convert_image(samples[i][index])\n",
        "      plt.imshow(img, animated=True)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.axis('off')\n",
        "\n",
        "    plt.savefig(file_path + 'image_at_timestep_{:04d}.png'.format(i))\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rKOukMPm3cL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import imageio"
      ],
      "metadata": {
        "id": "jMJAuk-w5Kiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'ddpm.gif'\n",
        "\n",
        "filenames = glob.glob(file_path + 'image*.png')\n",
        "filenames = sorted(filenames)\n",
        "images = []\n",
        "\n",
        "with imageio.get_writer(file_path + anim_file, mode='I') as writer:\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "metadata": {
        "id": "UU_9V_895NSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_docs"
      ],
      "metadata": {
        "id": "zSIJ-1f15eaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_docs.vis.embed as embed"
      ],
      "metadata": {
        "id": "6XwpRuk85gvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed.embed_file(file_path + anim_file)"
      ],
      "metadata": {
        "id": "bzHzYfr55juF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}